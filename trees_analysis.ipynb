{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c921482-5ec6-426b-a6b7-ad0dca7e577c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "687f9c85-9713-4be6-9ccf-51ee3ea495c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.corpus import stopwords\n",
    "from utils import pipeline, calculate_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38a46f23-5e11-4cff-ae03-45e63ab2bfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_data = \"./op_spam_v1.4/negative_polarity/\"\n",
    "min_df = 0.05  # Minimal document frequency of a word to be included in the vocabulary\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "val_size = None  # Size of the validation set as a percentage of the training set, set to None to disable\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9be3d2-0ca1-4290-b844-9b789a24caa0",
   "metadata": {},
   "source": [
    "## Using a classification tree and random forests on uni-grams\n",
    "We first train and tune both the classification tree and the random forest on the uni-grams of the data.\n",
    "\n",
    "We will use the cost-complexity pruning, max_depth and minimal sample leaf as the hyperparameters for the classification tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfdd0228-9f45-4339-b26a-16cbd36323ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (640, 326); y_train: 640\n",
      "X_test: (160, 326); y_test: 160\n",
      "Vocabulary size: 326\n"
     ]
    }
   ],
   "source": [
    "max_features = None  # Maximum vocab size\n",
    "ngram_range = (1, 1)  # Range of n-grams to include in the vocabulary\n",
    "\n",
    "# load in the data using the pipeline\n",
    "X_train, X_test, y_train, y_test, vectorizer = pipeline(fp_data, max_features, ngram_range, min_df,\n",
    "                                                                        stop_words=stop_words, val_size=val_size)\n",
    "# print the sizes of the train and test set\n",
    "print(f\"X_train: {X_train.shape}; y_train: {len(y_train)}\")\n",
    "# print(f\"X_val: {X_val.shape}; y_val: {len(y_val)}\")\n",
    "print(f\"X_test: {X_test.shape}; y_test: {len(y_test)}\")\n",
    "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f24131d-3d5c-4edc-a6b9-3b3c8f438d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_tree_params = {\n",
    "    \"max_depth\": [None, 10, 20, 30],\n",
    "    \"ccp_alpha\": [0.0, 0.05, 0.1, 0.2],\n",
    "    \"min_samples_leaf\": [1, 2, 4, 6]\n",
    "}\n",
    "\n",
    "rand_forest_params = {\n",
    "    \"n_estimators\": [50, 100, 150],\n",
    "    \"max_depth\": [None, 10, 20, 30],\n",
    "    \"max_features\": [None, 'sqrt', 0.25]\n",
    "}\n",
    "\n",
    "def tune_model(model, params, x, y):\n",
    "    clf = GridSearchCV(model, params, cv=5)\n",
    "    clf.fit(x, y)\n",
    "    return clf.best_params_\n",
    "\n",
    "def print_metrics(y_true, y_pred):\n",
    "    acc, pre, rec, f1 = calculate_metrics(np.asarray(y_test), np.asarray(y_pred))\n",
    "    return f'accuracy: {acc}; precision: {pre}; recall: {rec}; f1_score: {f1}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b08af744-d8cb-4b97-b84b-83a2b13fa8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters decision tree (uni-gram):\n",
      "accuracy: 0.64375; precision: 0.6419753086419753; recall: 0.65; f1_score: 0.6459627329192547\n",
      "best params found: {'ccp_alpha': 0.05, 'max_depth': None, 'min_samples_leaf': 1}\n"
     ]
    }
   ],
   "source": [
    "# training the uni-gram models (decision_tree)\n",
    "tree = DecisionTreeClassifier(random_state=random_state)\n",
    "best_params = tune_model(tree, class_tree_params, X_train, y_train)\n",
    "\n",
    "# train and evaluate the model with the best parameters\n",
    "bp_tree = DecisionTreeClassifier(random_state=random_state).set_params(**best_params).fit(X_train, y_train)\n",
    "y_pred = bp_tree.predict(X_test)\n",
    "print(f\"best parameters decision tree (uni-gram):\\n{print_metrics(y_test, y_pred)}\\nbest params found: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13af25d9-d882-405c-8c7e-54ad7f453e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters random forest (uni-gram):\n",
      "accuracy: 0.76875; precision: 0.7471264367816092; recall: 0.8125; f1_score: 0.778443113772455\n",
      "best params found: {'max_depth': 20, 'max_features': 'sqrt', 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# training the uni-gram models (random forests)\n",
    "rd_forest = RandomForestClassifier(random_state=random_state)\n",
    "best_params = tune_model(rd_forest, rand_forest_params, X_train, y_train)\n",
    "\n",
    "# train and evaluate the model with the best parameters\n",
    "bp_forest = RandomForestClassifier(random_state=random_state).set_params(**best_params).fit(X_train, y_train)\n",
    "y_pred = bp_forest.predict(X_test)\n",
    "print(f\"best parameters random forest (uni-gram):\\n{print_metrics(y_test, y_pred)}\\nbest params found: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39d622e1-4cb6-4f30-8866-4da24754e4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5502fdbd-5c80-4d42-8374-9eb8635add4f",
   "metadata": {},
   "source": [
    "## Using a classification tree and random forests on bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8138f36-6101-42db-9e0b-0801898baff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (640, 340); y_train: 640\n",
      "X_test: (160, 340); y_test: 160\n",
      "Vocabulary size: 340\n"
     ]
    }
   ],
   "source": [
    "ngram_range = (1, 2)  # Range of n-grams to include in the vocabulary\n",
    "\n",
    "# load in the data using the pipeline\n",
    "X_train, X_test, y_train, y_test, vectorizer = pipeline(fp_data, max_features, ngram_range, min_df,\n",
    "                                                                        stop_words=stop_words, val_size=val_size)\n",
    "# print the sizes of the train and test set\n",
    "print(f\"X_train: {X_train.shape}; y_train: {len(y_train)}\")\n",
    "# print(f\"X_val: {X_val.shape}; y_val: {len(y_val)}\")\n",
    "print(f\"X_test: {X_test.shape}; y_test: {len(y_test)}\")\n",
    "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edb109c9-8b17-4055-87b7-818832ee8880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters decision tree (bi-gram):\n",
      "accuracy: 0.64375; precision: 0.6419753086419753; recall: 0.65; f1_score: 0.6459627329192547\n",
      "best params found: {'ccp_alpha': 0.05, 'max_depth': None, 'min_samples_leaf': 1}\n"
     ]
    }
   ],
   "source": [
    "# training the bi-gram models (decision_tree)\n",
    "tree = DecisionTreeClassifier(random_state=random_state)\n",
    "best_params = tune_model(tree, class_tree_params, X_train, y_train)\n",
    "\n",
    "# train and evaluate the model with the best parameters\n",
    "bp_tree = DecisionTreeClassifier(random_state=random_state, **best_params).fit(X_train, y_train)\n",
    "y_pred = bp_tree.predict(X_test)\n",
    "print(f\"best parameters decision tree (bi-gram):\\n{print_metrics(y_test, y_pred)}\\nbest params found: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98d2c668-3ab6-4a67-b2ad-0bd0bb76d646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters random forest (bi-gram):\n",
      "accuracy: 0.725; precision: 0.7142857142857143; recall: 0.75; f1_score: 0.7317073170731706\n",
      "best params found: {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# training the bi-gram models (random forests)\n",
    "rd_forest = RandomForestClassifier(random_state=random_state)\n",
    "best_params = tune_model(rd_forest, rand_forest_params, X_train, y_train)\n",
    "\n",
    "# train and evaluate the model with the best parameters\n",
    "bp_forest = RandomForestClassifier(random_state=random_state, **best_params).fit(X_train, y_train)\n",
    "y_pred = bp_forest.predict(X_test)\n",
    "print(f\"best parameters random forest (bi-gram):\\n{print_metrics(y_test, y_pred)}\\nbest params found: {best_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
